{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement du modèle depuis le fichier local : C:\\Users\\marti\\Desktop\\Nouveau dossier\\Homework_9\\model_2024_hairstyle.keras\n",
      "Modèle chargé avec succès.\n",
      "Conversion en modèle TensorFlow Lite...\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\marti\\AppData\\Local\\Temp\\tmpjzlteynq\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\marti\\AppData\\Local\\Temp\\tmpjzlteynq\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\marti\\AppData\\Local\\Temp\\tmpjzlteynq'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 200, 200, 3), dtype=tf.float32, name='input_layer')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2094987922944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2094523337888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2093964045200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2093964043616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2093962791744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2094523425088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "Modèle TensorFlow Lite sauvegardé avec succès : C:\\Users\\marti\\Desktop\\Nouveau dossier\\Homework_9\\model_2024_hairstyle.tflite\n",
      "Taille du modèle TensorFlow Lite converti : 76.58 MB\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# Étape 1 : Définir le chemin du modèle local\n",
    "model_path = r\"C:\\Users\\marti\\Desktop\\Nouveau dossier\\Homework_9\\model_2024_hairstyle.keras\"\n",
    "\n",
    "# Vérification que le fichier existe\n",
    "try:\n",
    "    print(f\"Chargement du modèle depuis le fichier local : {model_path}\")\n",
    "    model_keras = tf.keras.models.load_model(model_path)\n",
    "    print(\"Modèle chargé avec succès.\")\n",
    "except Exception as e:\n",
    "    print(f\"Erreur lors du chargement du modèle : {e}\")\n",
    "    exit()  # Quittez si le chargement échoue\n",
    "\n",
    "# Étape 2 : Conversion en modèle TensorFlow Lite\n",
    "print(\"Conversion en modèle TensorFlow Lite...\")\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_keras)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Étape 3 : Sauvegarder le modèle converti dans le même répertoire\n",
    "output_dir = os.path.dirname(model_path)  # Obtenir le répertoire du modèle original\n",
    "output_path = os.path.join(output_dir, \"model_2024_hairstyle.tflite\")\n",
    "\n",
    "try:\n",
    "    with open(output_path, \"wb\") as f:\n",
    "        f.write(tflite_model)\n",
    "    print(f\"Modèle TensorFlow Lite sauvegardé avec succès : {output_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Erreur lors de la sauvegarde du modèle : {e}\")\n",
    "    exit()\n",
    "\n",
    "# Étape 4 : Afficher la taille du modèle TF-Lite\n",
    "model_size_in_mb = len(tflite_model) / (1024 * 1024)\n",
    "print(f\"Taille du modèle TensorFlow Lite converti : {model_size_in_mb:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output index: 13\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Charger le modèle TF-Lite\n",
    "interpreter = tf.lite.Interpreter(model_path=r\"C:\\Users\\marti\\Desktop\\Nouveau dossier\\Homework_9\\model_2024_hairstyle.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Récupérer les indices d'entrée et de sortie\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Afficher les détails de sortie\n",
    "print(f\"Output index: {output_details[0]['index']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First pixel (R channel): 0.24\n"
     ]
    }
   ],
   "source": [
    "from io import BytesIO\n",
    "from urllib import request\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Fonction pour télécharger une image\n",
    "def download_image(url):\n",
    "    with request.urlopen(url) as resp:\n",
    "        buffer = resp.read()\n",
    "    stream = BytesIO(buffer)\n",
    "    img = Image.open(stream)\n",
    "    return img\n",
    "\n",
    "# Fonction pour redimensionner l'image\n",
    "def prepare_image(img, target_size):\n",
    "    if img.mode != 'RGB':\n",
    "        img = img.convert('RGB')\n",
    "    img = img.resize(target_size, Image.NEAREST)\n",
    "    return img\n",
    "\n",
    "# Télécharger et redimensionner l'image\n",
    "image_url = \"https://habrastorage.org/webt/yf/_d/ok/yf_dokzqy3vcritme8ggnzqlvwa.jpeg\"\n",
    "img = download_image(image_url)\n",
    "img = prepare_image(img, (200, 200))  # Taille cible\n",
    "\n",
    "# Convertir en tableau numpy\n",
    "img_array = np.array(img) / 255.0  # Normalisation des pixels entre 0 et 1\n",
    "print(f\"First pixel (R channel): {img_array[0, 0, 0]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forme initiale de l'image : (200, 200, 3)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Forme initiale de l'image : {img_array.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_array = np.squeeze(img_array)  # Supprime les dimensions de taille 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_array = np.expand_dims(img_array, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_array = img_array.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forme corrigée de l'image : (1, 200, 200, 3)\n",
      "Forme attendue par le modèle : [  1 200 200   3]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Forme corrigée de l'image : {img_array.shape}\")\n",
    "print(f\"Forme attendue par le modèle : {input_details[0]['shape']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output: [[0.8937741]]\n"
     ]
    }
   ],
   "source": [
    "# Exécuter le modèle\n",
    "interpreter.set_tensor(input_details[0]['index'], img_array)\n",
    "interpreter.invoke()\n",
    "\n",
    "# Obtenir les résultats\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "print(f\"Model output: {output_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\marti\\desktop\\code\\venv\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\marti\\desktop\\code\\venv\\lib\\site-packages (from requests) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\marti\\desktop\\code\\venv\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\marti\\desktop\\code\\venv\\lib\\site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\marti\\desktop\\code\\venv\\lib\\site-packages (from requests) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: requests\n",
      "Version: 2.32.3\n",
      "Summary: Python HTTP for Humans.\n",
      "Home-page: https://requests.readthedocs.io\n",
      "Author: Kenneth Reitz\n",
      "Author-email: me@kennethreitz.org\n",
      "License: Apache-2.0\n",
      "Location: c:\\users\\marti\\desktop\\code\\venv\\lib\\site-packages\n",
      "Requires: certifi, charset-normalizer, idna, urllib3\n",
      "Required-by: tensorflow-intel\n"
     ]
    }
   ],
   "source": [
    "!pip show requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
